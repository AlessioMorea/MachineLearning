@article{10.5555/2188385.2188395,
author = {Bergstra, James and Bengio, Yoshua},
title = {Random Search for Hyper-Parameter Optimization},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
number = {null},
issn = {1532-4435},
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
journal = {J. Mach. Learn. Res.},
month = {feb},
pages = {281–305},
numpages = {25},
keywords = {model selection, neural networks, response surface modeling, deep learning, global optimization}
}
@book{géron2017hands-on,
  added-at = {2018-04-06T05:58:31.000+0200},
  address = {Sebastopol, CA},
  author = {Géron, Aurélien},
  biburl = {https://www.bibsonomy.org/bibtex/2a91270a3a516f4edaa5d459c40317fcc/achakraborty},
  interhash = {e2bd4a803c6cba6cca1d926b393806ad},
  intrahash = {a91270a3a516f4edaa5d459c40317fcc},
  isbn = {978-1491962299},
  keywords = {2017 book machine-learning oreilly tensorflow textbook},
  publisher = {O'Reilly Media},
  timestamp = {2018-04-06T05:59:31.000+0200},
  title = {Hands-on machine learning with Scikit-Learn and TensorFlow : concepts, tools, and techniques to build intelligent systems},
  year = 2017
}

